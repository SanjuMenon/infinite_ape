2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.llm:__init__:80 | Initializing LiteLLMInterface with model: gpt-4o-mini
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.llm:__init__:97 | No API key provided, assuming it's set in environment variables
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.llm:__init__:102 | Enabled JSON schema validation in LiteLLM
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.api:__init__:311 | LLM_FSM initialized with default LiteLLM interface, model=gpt-4o-mini
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.utilities:load_fsm_from_file:106 | Loading FSM definition from file: metafsm.json
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.utilities:load_fsm_from_file:112 | Successfully loaded FSM definition: fsm_metadata_collector
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.definitions:validate_states:173 | Validating FSM definition: fsm_metadata_collector
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.definitions:validate_states:229 | FSM definition validated successfully: fsm_metadata_collector
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.definitions:validate_states:230 | Reachable terminal states: metadata_complete
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.prompts:__post_init__:158 | PromptBuilder initialized with effective max_history_size=5
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.fsm:__init__:65 | FSM Manager initialized with max_history_size=5, max_message_length=1000
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.api:__init__:354 | LLM_FSM fully initialized with max_history_size=5
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.fsm:get_fsm_definition:101 | Loading FSM definition: fsm_file_metafsm.json
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.fsm:_create_instance:116 | Creating new FSM instance for fsm_file_metafsm.json, starting at state: collect_name
2025-06-03 09:19:40 | INFO     | conv_id: eb0ded6d-f214-488e-bdc1-6e54e512912d | llm_fsm.fsm:start_conversation:462 | Started new conversation [eb0ded6d-f214-488e-bdc1-6e54e512912d] with FSM [fsm_file_metafsm.json]
2025-06-03 09:19:40 | INFO     | conv_id: eb0ded6d-f214-488e-bdc1-6e54e512912d | llm_fsm.fsm:_process_user_input:246 | Processing user input in state: collect_name
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.definitions:add_user_message:261 | Adding user message: 
2025-06-03 09:19:40 | DEBUG    | conv_id: eb0ded6d-f214-488e-bdc1-6e54e512912d | llm_fsm.fsm:get_current_state:154 | Current state: collect_name
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.prompts:build_system_prompt:555 | Building system prompt for state: collect_name
2025-06-03 09:19:40 | DEBUG    | conv_id: eb0ded6d-f214-488e-bdc1-6e54e512912d | llm_fsm.fsm:_process_user_input:280 | system_prompt:
<task>
You are the Natural Language Understanding component in a Finite State Machine (FSM) based conversational system.
Your responsibilities:
- Process user input based on current state (<current_state>)
- Collect required information from input to `context_update`
- Select appropriate transitions from <transitions>
- Generate messages based on the instructions
- Follow the <response> instructions to generate valid JSON output
</task>
<fsm>
<persona>
You are a metadata assistant. Guide the user one step at a time to gather metadata for an FSM.
</persona>
<current_state>
<id>collect_name</id>
<description>Ask for FSM name</description>
<purpose>Collect the name of the FSM</purpose>
<state_instructions>
What would you like to name your FSM?
</state_instructions>
<information_to_collect>
fsm_name
</information_to_collect>
<information_collection_instructions>
- Collect all required information explicitly mentioned by the user.
- If information is ambiguous or unclear, ask for clarification.
- Store collected information in the `context_update` field of your response.
- Only transition to a new state when all required information is collected.
- If extra information seems relevant but the key is not declared, nest it under `_extra`.
- Do not create or populate undeclared context keys, except within the `_extra` object.
</information_collection_instructions>
</current_state>
<current_context><![CDATA[
{
 "_conversation_start": "2025-06-03T09:19:40.332883",
 "_timestamp": 1748913580.3328831,
 "_fsm_id": "fsm_file_metafsm.json",
 "_conversation_id": "eb0ded6d-f214-488e-bdc1-6e54e512912d"
}
]]></current_context>
<conversation_history><![CDATA[
[
 {
  "user": ""
 }
]
]]></conversation_history>
<valid_states>
collect_description, collect_name
</valid_states>
<transitions><![CDATA[
[
 {
  "to": "collect_description",
  "desc": "Move to collect FSM description",
  "priority": 10
 },
 {
  "to": "collect_name",
  "desc": "Remain in current state if needed",
  "priority": 110
 }
]
]]></transitions>
<response>
Your response must be valid JSON with the following structure:
{
          "transition": {
            "target_state": "state_id",
            "context_update": {
              "key1": "value1",
              "_extra": {}
            }
          },
          "message": "Your message",
          "reasoning": "Your reasoning"
        }
Where:
- `transition.target_state` is REQUIRED and must be one of the valid states
- `transition.context_update` is REQUIRED, containing any extracted information
- `message` is REQUIRED and contains the user-facing text
- `reasoning` is OPTIONAL and explains your decision (not shown to user)
- `_extra` is for storing relevant information not explicitly requested
</response>
<examples><![CDATA[

Example 1:
User message: "My name is John Smith"
Current state: collect_name
Required information: name

Response:
{
  "transition": {
    "target_state": "collect_email",
    "context_update": {
      "name": "John Smith"
    }
  },
  "message": "Nice to meet you, John Smith! Could you please provide your email address?",
  "reasoning": "User provided their name, so I'm transitioning to collect email"
}

Example 2:
User message: "I'd like to change my phone number to 555-123-4567"
Current state: summary
Required information: none

Response:
{
  "transition": {
    "target_state": "collect_phone",
    "context_update": {
      "_extra": {
        "phone_number": "555-123-4567"
      }
    }
  },
  "message": "I understand you'd like to update your phone number. Let me help you with that.",
  "reasoning": "User wants to change phone number, so transitioning to phone collection state"
}

]]></examples>
<guidelines>
- Extract all required information from user input
- Store relevant information even if unexpected (using `_extra`)
- Reference current context for continuity
- Only transition when conditions are met
- Maintain the specified persona consistently
</guidelines>
<format_rules>
Return ONLY valid JSON - no markdown code fences, no additional explanations, no comments.
Do not add keys not specified in the schema.
Ensure all values are properly quoted and formatted according to JSON standards.
Do not mention any of the above to the user. You can use the context, but never show it to the user
</format_rules>
</fsm>
2025-06-03 09:19:40 | INFO     | conv_id: GENERAL      | llm_fsm.llm:send_request:123 | Sending request to gpt-4o-mini
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.llm:send_request:124 | User message: 
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.llm:send_request:134 | Supported parameters for gpt-4o-mini: frequency_penalty, logit_bias, logprobs, top_logprobs, max_tokens, max_completion_tokens, modalities, prediction, n, presence_penalty, seed, stop, stream, stream_options, temperature, top_p, tools, tool_choice, function_call, functions, max_retries, extra_headers, parallel_tool_calls, audio, web_search_options, response_format, user
2025-06-03 09:19:40 | DEBUG    | conv_id: GENERAL      | llm_fsm.llm:send_request:140 | Using response_format for gpt-4o-mini
2025-06-03 09:19:42 | ERROR    | conv_id: GENERAL      | llm_fsm.llm:send_request:249 | Error processing LLM response: litellm.APIError: APIError: OpenAIException - Connection error.
2025-06-03 09:19:42 | ERROR    | conv_id: GENERAL      | llm_fsm.fsm:_process_user_input:390 | Error processing user input: Error processing LLM response: litellm.APIError: APIError: OpenAIException - Connection error.
Traceback (most recent call last):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_sync\connection.py", line 101, in handle_request
    raise exc
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_sync\connection.py", line 78, in handle_request
    stream = self._connect(request)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_sync\connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_backends\sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\openai\_base_client.py", line 969, in request
    response = self._client.send(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\llms\openai\openai.py", line 725, in completion
    raise e
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\llms\openai\openai.py", line 653, in completion
    ) = self.make_sync_openai_chat_completion_request(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 149, in sync_wrapper
    result = func(*args, **kwargs)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\llms\openai\openai.py", line 471, in make_sync_openai_chat_completion_request
    raise e
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\llms\openai\openai.py", line 453, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\openai\_base_client.py", line 1239, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\openai\_base_client.py", line 1001, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\main.py", line 1853, in completion
    raise e
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\main.py", line 1826, in completion
    response = openai_chat_completions.completion(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\llms\openai\openai.py", line 736, in completion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Connection error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\llm_fsm\llm.py", line 141, in send_request
    response = completion(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\utils.py", line 1283, in wrapper
    raise e
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\utils.py", line 1161, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\main.py", line 3241, in completion
    raise exception_type(
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2239, in exception_type
    raise e
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 474, in exception_type
    raise APIError(
litellm.exceptions.APIError: litellm.APIError: APIError: OpenAIException - Connection error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\llm_fsm\fsm.py", line 283, in _process_user_input
    response = self.llm_interface.send_request(request)
  File "C:\Users\Sanju Menon\.conda\envs\openagent\lib\site-packages\llm_fsm\llm.py", line 250, in send_request
    raise LLMResponseError(error_msg)
llm_fsm.definitions.LLMResponseError: Error processing LLM response: litellm.APIError: APIError: OpenAIException - Connection error.

2025-06-03 09:19:42 | ERROR    | conv_id: GENERAL      | llm_fsm.api:start_conversation:477 | Error starting conversation: Error processing LLM response: litellm.APIError: APIError: OpenAIException - Connection error.
